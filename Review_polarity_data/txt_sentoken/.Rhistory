doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
neg_words = append(neg_words, unique_tokens)
}
for (i in 1:5){
doc_name = docs[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
neg_words = append(neg_words, unique_tokens)
}
for (i in 1:length(docs)){
doc_name = docs[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
neg_words = append(neg_words, unique_tokens)
}
docs <- list.files(paste(wd, sentiments[2], sep = "/"))
neg_docs <- list.files(paste(wd, sentiments[1], sep = "/"))
pos_docs <- list.files(paste(wd, sentiments[2], sep = "/"))
neg_words <- c()
pos_words <- c()
neg_train_index <- sample(1:length(neg_docs),ceiling(length(neg_docs) * .6))
set.seed(666)
neg_docs <- list.files(paste(wd, sentiments[1], sep = "/"))
pos_docs <- list.files(paste(wd, sentiments[2], sep = "/"))
neg_train_index <- sample(1:length(neg_docs),ceiling(length(neg_docs) * .6))
pos_train_index <- sample(1:length(pos_docs),ceiling(length(pos_docs) * .6))
neg_training <- neg_docs[neg_train_index]
pos_training <- pos_docs[pos_train_index]
neg_words <- c()
pos_words <- c()
for (i in 1:length(neg_training)){
doc_name = neg_training[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
neg_words = append(pos_words, unique_tokens)
}
for (i in 1:length(neg_training)){
doc_name = neg_training[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
neg_words = append(neg_words, unique_tokens)
}
for (i in 1:length(pos_training)){
doc_name = pos_training[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
pos_words = append(pos_words, unique_tokens)
}
for (i in 1:length(pos_training)){
doc_name = pos_training[i]
file_location = paste(wd, sentiments[2], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
pos_words = append(pos_words, unique_tokens)
}
pos_training <- pos_docs[-pos_train_index]
pos_training <- pos_docs[pos_train_index]
neg_testing <- neg_docs[-neg_train_index]
pos_testing <- pos_docs[-pos_train_index]
neg_prior = length(neg_docs) / sum(length(neg_docs) + length(pos_docs))
pos_prior = length(pos_docs) / sum(length(neg_docs) + length(pos_docs))
neg_posterior = numeric()
pos_posterior = numeric()
makePosterior <- function(file_vector, words_vector) {
for (i in length(file_vector)) {
doc_name = file_vector[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (i in length(unique_tokens)) {
one_string = paste(words_vector, collapse = " ")
word = unique_tokens[i]
word_count = str_count(one_string, word) + 1
total_words = length(words_vector) + length(unique(words_vector))
posterior = append(posterior, word_count / total_words)
}
}
}
neg_posterior <- makePosterior(neg_testing, neg_words)
posterior = numeric()
neg_posterior <- makePosterior(neg_testing, neg_words)
makePosterior <- function(file_vector, words_vector) {
for (i in length(file_vector)) {
doc_name = file_vector[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (i in length(unique_tokens)) {
one_string = paste(words_vector, collapse = " ")
word = unique_tokens[i]
word_count = str_count(one_string, word) + 1
total_words = length(words_vector) + length(unique(words_vector))
posterior = append(posterior, word_count / total_words)
}
}
}
neg_posterior <- makePosterior(neg_testing, neg_words)
doc_name = neg_testing[1]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
one_string = paste(words_vector, collapse = " ")
one_string = paste(neg_words, collapse = " ")
word = unique_tokens[1]
word_count = str_count(one_string, word) + 1
total_words = length(words_vector) + length(unique(words_vector))
total_words = length(neg_words) + length(unique(words_vector))
neg_size < length(neg_words)
neg_size <- length(neg_words)
neg_vocab_size <- length(unique(neg_words))
total_words = neg_size + neg_vocab_size
9999 / total_words
neg_string = paste(neg_words, collapse = " ")
neg_string = paste(neg_words, collapse = " ")
pos_string = paste(pos_words, collapse = " ")
maya <- c("a", "b", "c")
paste(maya, collapse = " ")
rm(one_string)
testing <- c(neg_testing, pos_testing)
neg_prior = length(neg_docs) / sum(length(neg_docs) + length(pos_docs))
pos_prior = length(pos_docs) / sum(length(neg_docs) + length(pos_docs))
neg_string = paste(neg_words, collapse = " ")
pos_string = paste(pos_words, collapse = " ")
neg_denom = length(neg_words) + length(unique(neg_words))
pos_denom = length(pos_words) + length(unique(pos_words))
neg_sentiment_predictions = rep(0, length(neg_testing))
pos_sentiment_predictions = rep(1, length(pos_testing))
for (i in length(neg_testing)) {
doc_name = neg_testing[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (i in length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
}
neg_prob = neg_prior * prod(neg_posterior)
pos_prob = pos_prior * prod(pos_posterior)
if (neg_prob < pos_prob) {
neg_sentiment_predictions[i] = 1
}
}
sum(neg_sentiment_predictions)
doc_name = neg_testing[1]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
word = unique_tokesn[1]
word = unique_tokens[1]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = numeric()
pos_posterior = numeric()
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
neg_prob = neg_prior * prod(neg_posterior)
pos_prob = pos_prior * prod(pos_posterior)
if (neg_prob < pos_prob) {
neg_sentiment_predictions[i] = 1
}
for (i in length(neg_testing)) {
doc_name = neg_testing[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (i in length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
}
neg_prob = neg_prior * prod(neg_posterior)
pos_prob = pos_prior * prod(pos_posterior)
if (neg_prob < pos_prob) {
neg_sentiment_predictions[i] = 1
}
neg_posterior = numeric()
pos_posterior = numeric()
}
sum(neg_sentiment_predictions)
which.mac(neg_sentiment_predictions)
which.max(neg_sentiment_predictions)
doc_name = neg_testing[237]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (i in length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
}
length(unique_tokens)
word = unique_tokens[1]
neg_posterior = numeric()
pos_posterior = numeric()
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
word = unique_tokens[2]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
for (i in length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
}
neg_posterior = numeric()
pos_posterior = numeric()
for (i in length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = append(neg_posterior, neg_word_count / neg_denom)
pos_posterior = append(pos_posterior, pos_word_count / pos_denom)
}
length(pos_posterior)
doc_name = neg_testing[1]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (i in length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = c(neg_posterior, neg_word_count / neg_denom)
pos_posterior = c(pos_posterior, pos_word_count / pos_denom)
}
neg_posterior = numeric()
pos_posterior = numeric()
for (i in 1:length(unique_tokens)) {
word = unique_tokens[i]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = c(neg_posterior, neg_word_count / neg_denom)
pos_posterior = c(pos_posterior, pos_word_count / pos_denom)
}
neg_prob = neg_prior * prod(neg_posterior)
pos_prob = pos_prior * prod(pos_posterior)
neg_prob < pos_prob
neg_prob > pos_prob
if (neg_prior * prod(neg_posterior) < pos_prior * prod(pos_posterior)) {
print(YES!)
}
if (neg_prior * prod(neg_posterior) < pos_prior * prod(pos_posterior)) {
print("YES!")
}
if (neg_prior * prod(neg_posterior) > pos_prior * prod(pos_posterior)) {
print("YES!")
}
if (neg_prior * prod(neg_posterior) = pos_prior * prod(pos_posterior)) {
print("YES!")
}
if (neg_prior * prod(neg_posterior) == pos_prior * prod(pos_posterior)) {
print("YES!")
}
neg_sentiment_predictions = rep(0, length(neg_testing))
for (i in length(neg_testing)) {
doc_name = neg_testing[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (j in 1:length(unique_tokens)) {
word = unique_tokens[j]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = c(neg_posterior, neg_word_count / neg_denom)
pos_posterior = c(pos_posterior, pos_word_count / pos_denom)
}
pos_prob =
if (neg_prior * sum(log(neg_posterior)) < pos_prior * sum(log(pos_posterior)) {
neg_sentiment_predictions[i] = 1
}
neg_posterior = numeric()
pos_posterior = numeric()
}
for (i in length(neg_testing)) {
doc_name = neg_testing[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (j in 1:length(unique_tokens)) {
word = unique_tokens[j]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = c(neg_posterior, neg_word_count / neg_denom)
pos_posterior = c(pos_posterior, pos_word_count / pos_denom)
}
if (neg_prior * sum(log(neg_posterior)) < pos_prior * sum(log(pos_posterior)) {
neg_sentiment_predictions[i] = 1
}
neg_posterior = numeric()
pos_posterior = numeric()
}
for (i in length(neg_testing)) {
doc_name = neg_testing[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (j in 1:length(unique_tokens)) {
word = unique_tokens[j]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = c(neg_posterior, neg_word_count / neg_denom)
pos_posterior = c(pos_posterior, pos_word_count / pos_denom)
}
if (neg_prior * sum(log(neg_posterior)) < pos_prior * sum(log(pos_posterior))) {
neg_sentiment_predictions[i] = 1
}
neg_posterior = numeric()
pos_posterior = numeric()
}
sum(neg_sentiment_predictions)
which.max(neg_sentiment_predictions)
for (i in 1:length(neg_testing)) {
doc_name = neg_testing[i]
file_location = paste(wd, sentiments[1], doc_name, sep = "/")
doc = readChar(file_location, file.info(file_location)$size)
doc = gsub("[^[:alnum:][:space:]]", "", doc)
doc = gsub("\n", "", doc)
doc = gsub(" +", " ", doc)
tokenized_doc = MC_tokenizer(doc)
duplicates = duplicated(tokenized_doc)
unique_tokens = tokenized_doc[!duplicates]
for (j in 1:length(unique_tokens)) {
word = unique_tokens[j]
neg_word_count = str_count(neg_string, word) + 1
pos_word_count = str_count(pos_string, word) + 1
neg_posterior = c(neg_posterior, neg_word_count / neg_denom)
pos_posterior = c(pos_posterior, pos_word_count / pos_denom)
}
if (neg_prior * sum(log(neg_posterior)) < pos_prior * sum(log(pos_posterior))) {
neg_sentiment_predictions[i] = 1
}
neg_posterior = numeric()
pos_posterior = numeric()
}
sum(neg_sentiment_predictions)
neg_sentiment_predictions
wd <- "/Users/mfrasco/Documents/Non-School/Data Science/Text Classification/Review_polarity_data/txt_sentoken"
sentiments <- c("neg", "pos")
neg_docs <- list.files(paste(wd, sentiments[1], sep = "/"))
pos_docs <- list.files(paste(wd, sentiments[2], sep = "/"))
set.seed(666)
neg_train_index <- sample(1:length(neg_docs),ceiling(length(neg_docs) * .6))
pos_train_index <- sample(1:length(pos_docs),ceiling(length(pos_docs) * .6))
neg_training <- neg_docs[neg_train_index]
pos_training <- pos_docs[pos_train_index]
neg_testing <- neg_docs[-neg_train_index]
pos_testing <- pos_docs[-pos_train_index]
library(NLP)
library(tm)
library(stringr)
library(stringi)
file_directory <- paste(wd, sentiments[1], sep = "/")
neg_corpus <- Corpus(DirSource(directory = file_directory))
neg_corpus
maya <- neg_corpus[1]
class(maya)
str(maya)
maya$content
neg_corpus <- tm_map(neg_courpus, removePunctuation)
neg_corpus <- tm_map(neg_corpus, removePunctuation)
neg_corpus <- tm_map(neg_corpus, stripWhitespace, mc.cores=1)
neg_corpus <- tm_map(neg_corpus, content_transformer(tolower))
neg_corpus <- tm_map(neg_corpus, removeWords, stopwords("english"), mc.cores=1)
maya <- neg_corpus[1]
maya$content
neg_corpus <- tm_map(neg_corpus, stripWhitespace, mc.cores=1)
maya <- neg_corpus[1]
maya$content
duplicates <- duplicated(neg_corpus)
str(duplicates)
str(neg_corpus)
class(neg_corpus)
summary(neg_corpus)
maya$content
neg_corpus
size(corpus)
str(corpus)
size(neg_corpus)
length(neg_corpus)
neg_corpus[10]$content
class(neg_corpus[10]$content)
maya <- neg_corpus[10]$content
maya[1]
maya[[1]]
class(neg_corpus[[10]]$content)
maya <- neg_corpus[[10]]$content
maya
neg_corpus[10]
str(neg_corpus[10])
neg_corpus[[10]]
neg_corpus[[10]]$content
class(neg_corpus[[10]]$content)
str(neg_corpus[[10]]$content)
maya <- neg_corpus[[10]]$content
class(maya)
maya
maya2 <- paste(maya, collapse = "")
maya2
review = neg_corpus[[10]]$content
review = paste(review, collapse = "")
tokenized_review = MC_tokenizer(review)
duplicates = duplicated(tokenized_review)
unique_tokenized = tokenized_review[!duplicates]
unique_tokenized
install.packages('e1071', dependencies = TRUE)
