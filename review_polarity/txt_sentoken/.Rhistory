types_logical <- (storm_types < 4)
other_storms <- names(storm_types[types_logical])
other_storms
clean_data[1,3] %in% other_storms
clean_data[100,3] %in% other_storms
clean_data[1000,3] %in% other_storms
clean_data[1000,3]
clean_data <- read.csv('cleaned_refine_data.csv', stringsAsFactors = FALSE)
length(unique(clean_data$EVTYPE))
storm_types <- table(clean_data$EVTYPE)
types_logical <- (storm_types < 4)
other_storms <- names(storm_types[types_logical])
for (i in 1:nrow(clean_data)){
if (clean_data$EVTYPE[i] %in% other_storms){
clean_data$EVTYPE[i] = "OTHER"
}
}
length(unique(clean_data$EVTYPE))
storm_types <- table(clean_data$EVTYPE)
storm_types
clean_data$EVTYPE <- sub("FREEZING RAIN", "WINTER WEATHER", clean_data$EVTYPE)
length(unique(clean_data$EVTYPE))
length(unique(clean_data$FATALITIES))
table(clean_data$FATALITIES)
class(clean_data$FATALITIES)
clean_data$"PROPERTY DAMAGE" <- as.numeric()
names(clean_data)
clean_data$"PROPERTY DAMAGE" <- numeric()
clean_data[,11] <- numeric()
clean_data[,11:12] <- numeric()
clean_data[,11:12] <- c(numeric(), numeric())
clean_data[,11] <- numeric()
clean_data[,12] <- numeric()
names(clean_data)[11:12] <- c("PROPERTY DAMAGE", "CROP DAMAGE")
table(clean_data$PROPDMGEXP)
test <- storm_data$PROPDMGEXP == "-"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "?"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "+"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "0"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "1"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "2"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "3"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "4"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "5"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "6"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "7"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "8"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "B"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "m"
storm_data$REMARKS[test]
test <- storm_data$PROPDMGEXP == "-"
storm_data$REMARKS[test]
clean_data <- read.csv('clean_data2.csv', stringsAsFactors = FALSE)
length(unique(clean_data$EVTYPE))
clean_data <- read.csv('clean_data.csv', stringsAsFactors = FALSE)
length(unique(clean_data$EVTYPE))
names(clean_data)
storm_types <- table(clean_data$EVTYPE)
types_logical <- (storm_types < 4)
other_storms <- names(storm_types[types_logical])
for (i in 1:nrow(clean_data)){
if (clean_data$EVTYPE[i] %in% other_storms){
clean_data$EVTYPE[i] = "OTHER"
}
}
length(unique(clean_data$EVTYPE))
class(clean_data$PROPDMG)
class(clean_data$PROPDMGEXP)
class(clean_data$CROPDMG)
class(clean_data$CROPDMGEXP)
clean_data$CROPDMGEXP <- as.numeric(clean_data$CROPDMGEXP)
class(clean_data$CROPDMGEXP)
clean_data[,11] <- numeric()
clean_data[,12] <- numeric()
names(clean_data)[11:12] <- c("PROPERTY DAMAGE", "CROP DAMAGE")
for (i in 1:nrow(clean_data)){
clean_data[,11] = clean_data$PROPDMG[i] * clean_data$PROPDMGEXP[i]
clean_data[,12] = clean_data$CROPDMG[i] * clean_data$CROPDMGEXP[i]
}
clean_data[1:10,11]
clean_data[,11] = clean_data$PROPDMG * clean_data$PROPDMGEXP
clean_data[,12] = clean_data$CROPDMG * clean_data$CROPDMGEXP
class(clean_data[,11])
class(clean_data$INJURIES)
class(clean_data$FATALITIES)
clean_data$FATALITIES <- as.numeric(clean_data$FATALITIES)
clean_data$INJURIES <- as.numeric(clean_data$INJURIES)
fatal_sum <- tapply(clean_data$FATALITIES, clean_data$EVTYPE, sum)
fatal_sum
maya <- c("a","b","b", "c","a", "b")
tapply(maya, c("a","b","c"), tabulate)
tapply(maya, c("a","b","c"), length())
tapply(maya, c("a","b","c"), length(maya))
fatal_count <- tapply(clean_data$FATALITIES, clean_data$EVTYPE, count)
library(plyr)
fatal_count <- tapply(clean_data$FATALITIES, clean_data$EVTYPE, count)
fatal_count
storm_count <- count(clean_data, clean_data$EVTYPE)
storm_count <- table(clean_data$EVTYPE)
storm_count
fatal_sum <- tapply(clean_data$FATALITIES, clean_data$EVTYPE, sum)
fatal_sum
as.vector(storm_count)
fatal_sum
storm_count <- as.vector(table(clean_data$EVTYPE))
fatal_sum <- as.vector(tapply(clean_data$FATALITIES, clean_data$EVTYPE, sum))
fatal_max <- as.vector(tapply(clean_data$FATALITIES, clean_data$EVTYPE, max))
fatal_mean <- as.vector(tapply(clean_data$FATALITIES, clean_data$EVTYPE, mean))
fatal_median <- as.vector(tapply(clean_data$FATALITIES, clean_data$EVTYPE, median))
injury_sum <- as.vector(tapply(clean_data$INJURIES, clean_data$EVTYPE, sum))
injury_max <- as.vector(tapply(clean_data$INJURIES, clean_data$EVTYPE, max))
injury_mean <- as.vector(tapply(clean_data$INJURIES, clean_data$EVTYPE, mean))
injury_median <- as.vector(tapply(clean_data$INJURIES, clean_data$EVTYPE, median))
health_data_frame <- data.frame(sort(unique(clean_data$EVTYPE)), storm_count,
fatal_sum, fatal_max, fatal_mean, fatal_median,
injury_sum, injury_max, injury_mean, injury_median)
rm(storm_data)
health_data_frame
qplot(fatal_sum, fatal_max, size = fatal_mean, data = healt_data_frame)
library(ggplot2)
qplot(fatal_sum, fatal_max, size = fatal_mean, data = healt_data_frame)
qplot(fatal_sum, fatal_max, size = fatal_mean, data = health_data_frame)
qplot(fatal_sum, fatal_mean, size = fatal_max, data = health_data_frame)
ggplot(health_data_frame, aes(x = fatal_sum, y = fatal_mean, label = events)) + geom_point() + geom_text(aes(label=events),hjust=0,yjust=0)
events <- sort(unique(clean_data$EVTYPE))
health_data_frame <- data.frame(events, storm_count,
fatal_sum, fatal_max, fatal_mean, fatal_median,
injury_sum, injury_max, injury_mean, injury_median)
ggplot(health_data_frame, aes(x = fatal_sum, y = fatal_mean, label = events)) + geom_point() + geom_text(aes(label=events),hjust=0,yjust=0)
newsgroups <- c("comp.graphics", "comp.os.ms-windows.misc",
"comp.sys.ibm.pc.hardware", "comp.sys.mac.hardware",
"comp.windows.x", "rec.autos", "rec.motorcycles",
"rec.sport.baseball", "rec.sport.hockey", "sci.crypt"
"sci.electronics", "sci.med", "sci.space", "misc.forsale",
"talk.politics.misc", "talk.politics.guns",
"talk.politics.mideast", "talk.religion.misc",
"alt.atheism", "soc.religion.christian")
newsgroups <- c("comp.graphics", "comp.os.ms-windows.misc",
"comp.sys.ibm.pc.hardware", "comp.sys.mac.hardware",
"comp.windows.x", "rec.autos", "rec.motorcycles",
"rec.sport.baseball", "rec.sport.hockey", "sci.crypt"
"sci.electronics", "sci.med", "sci.space", "misc.forsale",
"talk.politics.misc", "talk.politics.guns",
"talk.politics.mideast", "talk.religion.misc",
"alt.atheism", "soc.religion.christian")
newsgroups <- c("comp.graphics", "comp.os.ms-windows.misc",
"comp.sys.ibm.pc.hardware", "comp.sys.mac.hardware",
"comp.windows.x", "rec.autos", "rec.motorcycles",
"rec.sport.baseball", "rec.sport.hockey", "sci.crypt",
"sci.electronics", "sci.med", "sci.space", "misc.forsale",
"talk.politics.misc", "talk.politics.guns",
"talk.politics.mideast", "talk.religion.misc",
"alt.atheism", "soc.religion.christian")
setwd("~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification")
setwd("~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification/20news-bydate")
wd <- "~/Documents/Non-School/Data Science/Natural Language Processing/
News Article Classification/20news-bydate"
wd
wd <- """
~/Documents/Non-School/Data Science/Natural Language Processing/
News Article Classification/20news-bydate
"""
library(tm)
library(plyr)
library(tau)
install.packages("tau")
library(tau)
wd <- "~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification/20news-bydate"
newsgroups <- c("comp.graphics", "comp.os.ms-windows.misc",
"comp.sys.ibm.pc.hardware", "comp.sys.mac.hardware",
"comp.windows.x", "rec.autos", "rec.motorcycles",
"rec.sport.baseball", "rec.sport.hockey", "sci.crypt",
"sci.electronics", "sci.med", "sci.space", "misc.forsale",
"talk.politics.misc", "talk.politics.guns",
"talk.politics.mideast", "talk.religion.misc",
"alt.atheism", "soc.religion.christian")
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus, stripWhitespace)
corpus_clean <- tm_map(corpus, tolower)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"))
return(corpus_clean)
}
BuildTDM <- function(directory = wd, newsgroup) {
file_directory <- paste("director", "newsgroup", sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, encoding = "ANSI"))
newscorpus_clean <- CleanCorpus
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm <- lapply(newsgroups, BuildTDM)
tdm <- lapply(newsgroups, BuildTDM, wd)
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste("director", "newsgroup", sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, encoding = "ANSI"))
newscorpus_clean <- CleanCorpus
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm <- lapply(newsgroups, BuildTDM, wd)
tdm <- lapply(newsgroups, BuildTDM, directory = wd)
wd <- "~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification/20news-bydate/"
test_directory <- "20news-bydate-test/"
train_directory <- "20news-bydate-train/"
test_directory <- "~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification/20news-bydate/20news-bydate-test/"
train_directory <- "~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification/20news-bydate/20news-bydate-train/"
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste(directory, newsgroup, sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, encoding = "ANSI"))
newscorpus_clean <- CleanCorpus
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm <- lapply(newsgroups, BuildTDM, directory = train_directory)
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste(directory, newsgroup, sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, encoding = "UTF-8"))
newscorpus_clean <- CleanCorpus
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm <- lapply(newsgroups, BuildTDM, directory = train_directory)
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste(directory, newsgroup, sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, encoding = "UTF-8"))
newscorpus_clean <- CleanCorpus(newscorpus)
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm <- lapply(newsgroups, BuildTDM, directory = train_directory)
tdm1 <- BuildTDM(train_directory, "alt.atheism")
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, asPlain)
corpus_clean <- tm_map(corpus, removeSignature)
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus, stripWhitespace)
corpus_clean <- tm_map(corpus, tolower)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"))
return(corpus_clean)
}
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste(directory, newsgroup, sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, encoding = "UTF-8"))
newscorpus_clean <- CleanCorpus(newscorpus)
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm1 <- BuildTDM(train_directory, "alt.atheism")
getwd()
setwd("~/Documents/Non-School/Data Science/Natural Language Processing/News Article Classification/20news-bydate/20news-bydate-train")
sci.electr.train <- Corpus( DirSource (“sci.electronics”),
readerControl=list(reader=readNewsgroup, language=“en_US” ) )
corp1 <- Corpus( DirSource("sci.electronics"), readerControl=list(reader=readNewsgroup, language="en_US"))
corp1 <- Corpus( DirSource("sci.electronics"), readerControl=list(language="en_US"))
corp1
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste(directory, newsgroup, sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory, readerControl=list(language="en_US")))
newscorpus_clean <- CleanCorpus(newscorpus)
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm1 <- BuildTDM(train_directory, "alt.atheism")
BuildTDM <- function(directory, newsgroup) {
file_directory <- paste(directory, newsgroup, sep = "")
newscorpus <- Corpus(DirSource(directory = file_directory))
newscorpus_clean <- CleanCorpus(newscorpus)
tdm <- TermDocumentMatrix(newscorpus_clean)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(newsgroup = newsgroup, tdm = tdm)
return(result)
}
tdm1 <- BuildTDM(train_directory, "alt.atheism")
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, asPlain)
corpus_clean <- tm_map(corpus, removeSignature)
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus, stripWhitespace)
corpus_clean <- tm_map(corpus, tolower)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"))
return(corpus_clean)
}
tdm1 <- BuildTDM(train_directory, "alt.atheism")
file_directory = paste(train_directory, newsgroups[1], sep = "")
file_directory
corp1 <- Corpus(DirSource(file_directory))
corp1 <- Corpus(DirSource(directory = file_directory, encoding = "ANSI"))
corp1 <- Corpus(DirSource(directory = file_directory, encoding = "UTF-8"))
corp1
corp1[[1]]
corp1_clean <- CleanCorpus(corp1)
install.packages("SnowballC")
library(SnowballC)
corp2_clean <- CleanCorpus(corp1)
rm(corp1)
rm(corp1_clean)
rm(corp2_clean)
corp1 <- Corpus(DirSource(directory = file_directory, encoding = "UTF-8"))
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, asPlain, mc.cores=1)
corpus_clean <- tm_map(corpus, removeSignature, mc.cores=1)
corpus_clean <- tm_map(corpus, removePunctuation, mc.cores=1)
corpus_clean <- tm_map(corpus, stripWhitespace, mc.cores=1)
corpus_clean <- tm_map(corpus, tolower, mc.cores=1)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"), mc.cores=1)
return(corpus_clean)
}
corp2_clean <- CleanCorpus(corp1)
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, as.PlainTextDocument, mc.cores=1)
corpus_clean <- tm_map(corpus, removeSignature, mc.cores=1)
corpus_clean <- tm_map(corpus, removePunctuation, mc.cores=1)
corpus_clean <- tm_map(corpus, stripWhitespace, mc.cores=1)
corpus_clean <- tm_map(corpus, tolower, mc.cores=1)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"), mc.cores=1)
return(corpus_clean)
}
corp2_clean <- CleanCorpus(corp1)
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, removeSignature, mc.cores=1)
corpus_clean <- tm_map(corpus, removePunctuation, mc.cores=1)
corpus_clean <- tm_map(corpus, stripWhitespace, mc.cores=1)
corpus_clean <- tm_map(corpus, tolower, mc.cores=1)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"), mc.cores=1)
return(corpus_clean)
}
corp2_clean <- CleanCorpus(corp1)
library(tm)
corp2_clean <- CleanCorpus(corp1)
CleanCorpus <- function(corpus){
corpus_clean <- tm_map(corpus, removeSignature)
corpus_clean <- tm_map(corpus, removePunctuation, mc.cores=1)
corpus_clean <- tm_map(corpus, stripWhitespace, mc.cores=1)
corpus_clean <- tm_map(corpus, tolower, mc.cores=1)
corpus_clean <- tm_map(corpus, removewords, stopwords("english"), mc.cores=1)
return(corpus_clean)
}
corp2_clean <- CleanCorpus(corp1)
corp1
corpus1 <- tm_map(corp1, removePunctuation)
corpus1 <- tm_map(corp1, asPlain)
corpus1 <- tm_map(corp1, asPlain, mc.cores=1)
corpus1 <- tm_map(corp1, asPlainTextDocument, mc.cores=1)
corpus1 <- tm_map(corp1, as.PlainTextDocument, mc.cores=1)
install.packages("twitteR")
library(twitteR)
delta.tweets <- searhTwitter('@delta', 1500)
delta.tweets <- searchTwitter('@delta', 1500)
library(XML)
fileURL <- "http://www.w3schools.com/xml/simple.xml"
fileURL <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileURL, useInternal = TRUE)
str(doc)
summary(doc)
rootNode <- xmlRoot(doc)
str(rootNode)
xmlName(rootNode)
names(rootNode)
rootNode[2]
rootNode[[2]]
rootNode[[2]][1]
rootNode[[2]][[1]]
xmlSApply(rootNode,xmlValue)
xmlSApply(rootNode,"//name",xmlValue)
xmlSApply(rootNode,"//price",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)
names(rootNode)
xmlName(rootNode)
xmlNamespace(rootNode)
xmlAttrs(rootNode)
xmlValue(rootNode)
xmlChildren(rootNode)
xmlName(rootNode)
xmlValue(rootNode)
doc1 = smlTreeParse("pubmed.xml", useInternal = TRUE)
doc1 = xmlTreeParse("pubmed.xml", useInternal = TRUE)
doc <- xmlTreeParse("http://www.w3schools.com/xml/cd_catalog.xml", useInternal = TRUE)
xmlRoot(doc)
cddoc <- xmlTreeParse("http://www.w3schools.com/xml/cd_catalog.xml", useInternal = TRUE)
xmlRoot(cddoc)
plantdoc <- xmlTreeParse("http://www.w3schools.com/xml/plant_catalog.xml", useInternal = TRUE)
xmlRoot(plantdoc)
xmlname(plantdoc)
xmlName(plantdoc)
rootName(plantdoc)
nodeName(plantdoc)
cd_top_root <- xmlRoot(cddoc)
xmlName(cd_top_root)
plant_top_root <- xmlRoot(plantdoc)
xmlName(plant_top_root)
top_root_name <- xmlName(cd_top_root)
top_root_name
firstchildrennames <- names(cd_top_root)
firstchildrennames
names(cd_top_root)[1:3] <- c("a", "c", "d")
top <- xmlRoot(cddoc)
top_root_name <- xmlName(top)
firstchildrennames <- names(top)
firstchildrennames <- names(top)
firstchildrennames
top[1]
top[[1]]
top[[2]]
top[[2]][3]
top[[2]][[3]]
xmlValue(top[[2]][[3]])
class(xmlValue(top[[2]][[3]]))
names(top[[1]])
names(top[1])
names(top[[1]][[2]])
names(top[[1]][2])
names(top[[1]])
names(top[[1]][["COMPANY"]])
company <- (top[[1]][["COMPANY"]])
company
class(company)
xmlValue(company)
xmlSApply(top[[1]], xmlValue)
xmlSApply(top[[1:2]], xmlValue)
top[[1:3]]
top[[1:10]]
xmlSApply(top[[1]], xmlValue)
firstchild <- xmlSApply(top[[1]], xmlValue)
class(firstchild)
as.list(firstchild)
as.vector(firstchild)
xmlSApply(top, xmlValue)
xmlValue(top)
myapp = oauth_app("twitter",
key = "pz083YOsY7PBjayw33E1sEKey", secret="uzVTyRXKxRu5e4dVQKI6nNr26eL3LaRRGx3VYJ9wXqHkWbEEyc")
library(httr)
myapp = oauth_app("twitter",
key = "pz083YOsY7PBjayw33E1sEKey",
secret="uzVTyRXKxRu5e4dVQKI6nNr26eL3LaRRGx3VYJ9wXqHkWbEEyc")
sig = sign_oauth1.0(myapp,
token = "293148093-o3dD9RUprmm3KjAq8hJ6V0gnTO6EPe6y2C1qcKut",
token_secret = "DrA8G9QIW9vWPK6iuBadob6MmgoYwRSHwH0Uoj0IXQrhX")
test = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
tes
test
json1 = content(test)
json1
str(json1)
json2 <- jsonlite::fromJSON(toJSON(json1))
json1 = content(test, encoding = "UTF-8")
json2 <- jsonlite::fromJSON(toJSON(json1))
json1 = content(test, encoding = "utf8")
json2 <- jsonlite::fromJSON(toJSON(json1))
library(tm)
library(stringr)
library(plyr)
options(stringsAsFactors = FALSE)
setwd("~/Documents/Non-School/Data Science/Natural Language Processing/Sentiment Polarity/review_polarity/txt_sentoken")
sentiments <- c("neg", "pos")
wd <- "~/Documents/Non-School/Data Science/Natural Language Processing/Sentiment Polarity/review_polarity/txt_sentoken"
options(stringsAsFactors = FALSE)
sentiments <- c("neg", "pos")
wd <- "~/Documents/Non-School/Data Science/Natural Language Processing/Sentiment Polarity/review_polarity/txt_sentoken"
CleanCorpus <- function(corpus) {
clean_corpus <- tm_map(corpus, removePunctuation)
clean_corpus <- tm_map(clean_corpus, stripWhitespace)
clean_corpus <- tm_map(clean_corpus, tolower)
clean_corpus <- tm_map(clean_corpus, removewords, stopwords("english"))
return(clean_corpus)
}
BuildTDM <- function(sentiment, directory) {
file_directory <- paste(wd, "/", sentiment, sep = "")
corpus <- Corpus(DirSource(directory = file_directory, encoding = "ANSI"))
clean_corpus <- CleanCorpus(corpus)
tdm <- TermDocumentMatrix(clean_corpus)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(review = sentiment, termdocmatrix <- tdm)
return(result)
}
tdm <- lapply(sentiments, BuildTDM, directory = wd)
BuildTDM <- function(sentiment, directory) {
file_directory <- paste(wd, "/", sentiment, sep = "")
corpus <- Corpus(DirSource(directory = file_directory, encoding = "UTF-8"))
clean_corpus <- CleanCorpus(corpus)
tdm <- TermDocumentMatrix(clean_corpus)
tdm <- removeSparseTerms(tdm, 0.7)
result <- list(review = sentiment, termdocmatrix <- tdm)
return(result)
}
tdm <- lapply(sentiments, BuildTDM, directory = wd)
